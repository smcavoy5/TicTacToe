{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6b3558-0e20-4260-9b05-9bf4ba24d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [' '] * 9\n",
    "        self.current_player = 'X'\n",
    "\n",
    "    def print_board(self):\n",
    "        print('-------------')\n",
    "        for i in range(3):\n",
    "            print('|', self.board[i * 3], '|', self.board[i * 3 + 1], '|', self.board[i * 3 + 2], '|')\n",
    "            print('-------------')\n",
    "\n",
    "    def get_state(self):\n",
    "        return [1 if x == 'X' else -1 if x == 'O' else 0 for x in self.board]\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i for i, x in enumerate(self.board) if x == ' ']\n",
    "\n",
    "    def make_move(self, position):\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "    def check_winner(self):\n",
    "        winning_combinations = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "            [0, 4, 8], [2, 4, 6]\n",
    "        ]\n",
    "        for combo in winning_combinations:\n",
    "            if self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' ':\n",
    "                return self.board[combo[0]]\n",
    "        if ' ' not in self.board:\n",
    "            return 'Draw'\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [' '] * 9\n",
    "        self.current_player = 'X'\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in self.available_moves():\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        self.make_move(action)\n",
    "        winner = self.check_winner()\n",
    "        if winner == 'X':\n",
    "            return self.get_state(), 1, True  # X wins\n",
    "        elif winner == 'O':\n",
    "            return self.get_state(), -1, True  # O wins\n",
    "        elif winner == 'Draw':\n",
    "            return self.get_state(), 0, True  # Draw\n",
    "        else:\n",
    "            return self.get_state(), 0, False  # Continue playing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be0f3a7-7c3d-428f-a6e9-d7166aaf5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=1):\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.q_values = {}  # Dictionary to store Q-values\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_values.get((state, action), 0.0)\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.random() < self.epsilon:  # Exploration\n",
    "            return random.choice(available_actions)\n",
    "        else:  # Exploitation\n",
    "            best_actions = []\n",
    "            best_q_value = float('-inf')\n",
    "            for action in available_actions:\n",
    "                q_value = self.get_q_value(state, action)\n",
    "                if q_value > best_q_value:\n",
    "                    best_actions = [action]\n",
    "                    best_q_value = q_value\n",
    "                elif q_value == best_q_value:\n",
    "                    best_actions.append(action)\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        max_next_q_value = max([self.get_q_value(next_state, next_action) for next_action in next_state.available_moves()])\n",
    "        new_q_value = (1 - self.alpha) * self.get_q_value(state, action) + self.alpha * (reward + self.gamma * max_next_q_value)\n",
    "        self.q_values[(state, action)] = new_q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5bdb7b-d539-41e8-9e26-1908086c5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning_agent(num_episodes):\n",
    "    agent = QLearningAgent()\n",
    "    env = TicTacToe()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = tuple(env.board)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            available_actions = env.available_moves()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            env.make_move(action)\n",
    "            next_state = tuple(env.board)\n",
    "            reward = 0\n",
    "            \n",
    "            winner = env.check_winner()\n",
    "            if winner == 'X':\n",
    "                reward = 1\n",
    "                done = True\n",
    "            elif winner == 'O':\n",
    "                reward = -1\n",
    "                done = True\n",
    "            elif winner == 'Draw':\n",
    "                done = True\n",
    "                \n",
    "            agent.update_q_value(state, action, reward, TicTacToe())\n",
    "            state = next_state\n",
    "\n",
    "    return agent\n",
    "\n",
    "trained_agent = train_q_learning_agent(num_episodes=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2aa932-8c16-4ed3-96c4-ceaff1436925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_train_q_learning_agent(num_episodes):\n",
    "    agent = QLearningAgent()\n",
    "    env = TicTacToe()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        env.reset()\n",
    "        states_actions_rewards = []\n",
    "\n",
    "        while True:\n",
    "            state = tuple(env.board)\n",
    "            available_actions = env.available_moves()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            env.make_move(action)\n",
    "            next_state = tuple(env.board)\n",
    "            reward = 0\n",
    "            \n",
    "            winner = env.check_winner()\n",
    "            if winner == 'X':\n",
    "                reward = 1\n",
    "                break\n",
    "            elif winner == 'O':\n",
    "                reward = -1\n",
    "                break\n",
    "            elif winner == 'Draw':\n",
    "                break\n",
    "\n",
    "            states_actions_rewards.append((state, action, reward))\n",
    "            env.make_move(random.choice(env.available_moves()))\n",
    "\n",
    "        # Update Q-values using collected experience\n",
    "        for state, action, reward in states_actions_rewards:\n",
    "            agent.update_q_value(state, action, reward, TicTacToe())\n",
    "\n",
    "    return agent\n",
    "\n",
    "trained_agent = self_play_train_q_learning_agent(num_episodes=10000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6d55bbad-9999-41ee-adcd-1cf7d23fb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "import random\n",
    "\n",
    "class NeuralQAgent:\n",
    "    def __init__(self, input_size, output_size, epsilon=0.1, gamma=0.99):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state, available_moves, testing=False):\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return random.choice(available_moves)  # Exploration\n",
    "        else:\n",
    "            q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "\n",
    "            # Only select valid moves\n",
    "            while True:\n",
    "                if torch.argmax(q_values).item() in available_moves:\n",
    "                    return torch.argmax(q_values).item()  # Exploitation\n",
    "                else:\n",
    "                    q_values[torch.argmax(q_values).item()] = -math.inf\n",
    "\n",
    "    def update_q_values(self, states, actions, rewards, next_states, dones):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute predicted Q-values\n",
    "        predicted_q_values = self.q_network(states)\n",
    "        predicted_q_values = torch.gather(predicted_q_values, 1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * torch.max(self.q_network(next_states), dim=1)[0]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_function(predicted_q_values, target_q_values.detach())\n",
    "\n",
    "        # Update network weights\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_neural_q_agent(agent, env, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state, env.available_moves())\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        if episode % 10000 == 0:\n",
    "            print(episode)\n",
    "        agent.update_q_values(states, actions, rewards, next_states, dones)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1843eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "trained_agent = NeuralQAgent(9, 9)\n",
    "env = TicTacToe()\n",
    "\n",
    "train_neural_q_agent(trained_agent, env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e9326fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Your turn (Enter position 0-8): \n",
      "-------------\n",
      "|   |   | O |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Your turn (Enter position 0-8): \n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Your turn (Enter position 0-8): \n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "-------------\n",
      "You win!\n",
      "Game Over.\n"
     ]
    }
   ],
   "source": [
    "def play_against_agent(agent):\n",
    "    env = TicTacToe()\n",
    "\n",
    "    while True:\n",
    "        env.print_board()\n",
    "        print(\"Your turn (Enter position 0-8): \")\n",
    "        user_input = input()\n",
    "        try:\n",
    "            user_action = int(user_input)\n",
    "            if user_action not in env.available_moves():\n",
    "                print(\"Invalid move. Please try again.\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "            continue\n",
    "\n",
    "        env.make_move(user_action)\n",
    "        winner = env.check_winner()\n",
    "        if winner == 'X':\n",
    "            env.print_board()\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "        elif winner == 'Draw':\n",
    "            env.print_board()\n",
    "            print(\"It's a draw!\")\n",
    "            break\n",
    "        elif winner == 'O':\n",
    "            env.print_board()\n",
    "            print(\"You lost!\")\n",
    "            break\n",
    "\n",
    "        # Agent's turn\n",
    "        #state = tuple(env.board)\n",
    "        state = env.get_state()\n",
    "        available_actions = env.available_moves()\n",
    "        action = agent.choose_action(state, available_actions, testing=True)\n",
    "        env.make_move(action)\n",
    "        winner = env.check_winner()\n",
    "        if winner == 'O':\n",
    "            env.print_board()\n",
    "            print(\"You lost!\")\n",
    "            break\n",
    "        elif winner == 'Draw':\n",
    "            env.print_board()\n",
    "            print(\"It's a draw!\")\n",
    "            break\n",
    "\n",
    "    print(\"Game Over.\")\n",
    "\n",
    "# Example usage:\n",
    "play_against_agent(trained_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7390de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 16, 44)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def play_against_random(agent):\n",
    "    env = TicTacToe()\n",
    "    wins = 0\n",
    "    draws = 0 \n",
    "    loses = 0  \n",
    "    for game in range(100):\n",
    "        while True:\n",
    "            state = env.get_state()\n",
    "            available_actions = env.available_moves()\n",
    "            action = agent.choose_action(state, available_actions, testing=True)\n",
    "            env.make_move(action)\n",
    "            winner = env.check_winner()\n",
    "            if winner == 'X':\n",
    "                wins+=1\n",
    "                env.reset()\n",
    "                break\n",
    "            elif winner == 'Draw':\n",
    "                env.reset()\n",
    "                draws += 1\n",
    "                break\n",
    "            elif winner == 'O':\n",
    "                env.reset()\n",
    "                loses += 1\n",
    "                break\n",
    "\n",
    "            # Agent's turn\n",
    "            #state = tuple(env.board)\n",
    "            \n",
    "            env.make_move(random.choice(env.available_moves()))\n",
    "            winner = env.check_winner()\n",
    "            if winner == 'O':\n",
    "                loses += 1\n",
    "                break\n",
    "            elif winner == 'Draw':\n",
    "\n",
    "                draws += 1\n",
    "                env.reset()\n",
    "                break\n",
    "            elif winner == 'X':\n",
    "                wins+=1\n",
    "                env.reset()\n",
    "                break\n",
    "\n",
    "    return wins, draws, loses\n",
    "# Example usage:\n",
    "play_against_random(trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
